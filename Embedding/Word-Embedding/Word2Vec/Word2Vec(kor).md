<h1 align="center">Word2Vec</h1>

이 글은 Ont-Hot Encoding이 무엇인지 알고 있다는 전제하에 시작됩니다.

 > [🔍 One-Hot-Encoding이 무엇인지 확인하러 가기!](../One-Hot-Encoding/One-Hot-Encoding(kor).md)

앞서 소개한 원-핫 인코딩의 경우 구현이 간단하다는 장점이 있습니다. 하지만, 인공지능의 경우 많은 단어를 처리하면서 단어 간 유사도를 계산할 수 있어야 좋은 성능을 낼 수 있기 때문에 원-핫 인코딩 기법은 좋은 선택이 되지 못할 수도 있습니다. 따라서 분산 표현 형태의 단어 임베딩 모델을 사용할 것 입니다. 이번에는 신경망 기반 단어 임베딩의 방법 중 대표적인 Word2Vec 모델을 알아보겠습니다.


<h2 align="center">What is Word2Vec</h2>

**Word2Vec**은 2013년 구글에서 발표했으며, 가장 많이 사용하고 있는 단어 임베딩 모델입니다. 기존 신경망 기반의 단어 임베딩 모델이 비해 구조상의 차이는 크게 없으나, 계산량을 놀랄정도로 줄여 빠른 학습을 가능케 하였습니다. Word2Vec 모델은 CBOW(continuous bag-of-words)  와 skip-gran 두 가지 모델로 나눌 수 있습니다.

우선 Word2Vec 패키지를 다운받아야 합니다.

### Ubuntu

```
$ curl -O https://bootstrap.pypa.io/get-pip.py
$ $ python3 get-pip.py --user
$ pip install gensim
```

### Mac OS

```
$ pip3 install gensim
```

### Window

```
> pip install --upgrade pip
> pip install gensim
```


<h2 align="center">CBOW and skip-gram</h2>

위에서 설명했던 Word2Vec의 2가지 모델인 CBOW와 skip-gram을 분석해보도록 하겠습니다.

### CBOW

CBOW 모델은 맥락이라 표현되는 주변 단어들을 이용해 타깃 단어를 예측하는 신경망 모델입니다. 신경망의 입력을 주변 단어들로 구성하고 출력을 타깃 단어로 설정해 학습된 가중치 데이터를 임베딩 벡터로 사용합니다. CBOW 모델은 타깃 단어의 소실만 계산하면 도기 때문에 학습 속도가 빠르다는 장점이 있습니다.

### skip-gram

skip-gram 모델은 CBOW 모델과 반대로 하나의 타깃 단어를 이용해 주변 단어들을 예측하는 신경망 모델입니다. 입출력이 CBOW 모델과 반대로 되어있기 때문에 skip-gram 모델이 CBOW 모델에 비해 예측해야 하는 맥락이 많아집니다. 해서 단어 분산 표현력이 우수해 CBOW 모델에 비해 임베딩 품질이 우수합니다.


<h2 align="center">Word2Vec Model</h2>

Word2Vec 모델을 텐서플로우나 케라스 같은 신경망 라이브러리를 이용해 직접 구현할 수 있지만, 오픈소스 라이브러리가 이미 존재하며, 임베딩 품질도 나쁘지 않아 많이 사용하고 있는 **Genism** 패키지를 이용할 것입니다.

한국어 Word2Vec을 만들기 위해서는 한국어 말뭉치를 수집해야 합니다. 이 예제는 <strong>네이버 영화 리뷰 데이터(Naver Sentiment Movie Corpus, NSMC)</strong>를 이용하여 모델을 만들어 보겠습니다. 제가 사용할 [rating.txt](./codes/ratings.txt)는 구버전일 수 있으므로 최신 버전의 발뭉치 데이터를 원하시는 분은 [New-rating.txt](https://github.com/e9t/nsmc)에서 다운받으실 수 있습니다.

말뭉치 데이터는 양이 많아 모델을 학습하는데 시간이 다소 걸립니다. Word2Vec을 사용할 때마다 오랜 시간이 걸리는 모델 학습을 매번할 수 없으므로 각 단어의 임베딩 벡터가 설정되어 있는 모델을 파일로 저장할 것입니다.

 > 저는 rating.txt(19MB) 파일을 Word2Vec으로 학습하는데 254s 가 소요되었습니다. 보시는 분들의 컴퓨터 사양에 따라 학습시간이 다를 수 있으므로 여유있게 ☕(커피) 한잔 하고 오시면 모델 파일이 생성되어 있을 것입니다.

모델 학습이 완료되었다면, [laod_word2vec_model.py](./codes/nvmc-model/laod_word2vec_model.py) 와 같이 코드를 입력하여 실제로 단어 임베딩된 값과 벡터 공간상의 유사한 단어들을 확인해봅시다.

코드 작성 후 실행시키면:

```
corpus_total_words :  1076896
달력 :  [ 9.32990983e-02 -1.34993032e-01 -1.03150487e-01  4.69429232e-02
 -3.20427082e-02  8.99038613e-02  5.08054346e-02  3.48317266e-01
 -1.52057752e-01  1.65496647e-01 -9.49010924e-02 -4.26943362e-01
    ......
  3.09792638e-01 -1.64287269e-01 -3.22148092e-02 -1.22924365e-01
 -1.61299482e-01 -1.79409847e-01  1.38744682e-01  5.65950647e-02]
일요일 = 월요일  0.64878
안성기 = 배우    0.59924126
대기업 = 삼성    0.4369776
일요일 != 삼성   0.27672067
히어로 != 삼성   0.20656006
[('씨야', 0.7492166757583618), ('정려원', 0.7250934839248657), ('김유미', 0.7148603200912476), ('정재영', 0.7102662920951843), ('김흥국', 0.7097449898719788)]
[('더 울버린', 0.664757490158081), ('엑스맨', 0.6643115878105164), ('캐리비안의 해적', 0.6600803732872009), ('편', 0.6376581788063049), ('X맨', 0.6375453472137451)]
```

와 같은 결과가 뜨는 것을 확인하실 수 있습니다.

 > 모델 학습 시 신경망 내 각 노드의 초기 가중치가 랜덤으로 설정됩니다. 따라서 모델 학습을 할 때 마다 단어의 임베딩 벡터값이 변경됩니다. 따라서 결과를 비교하실 때, 제 결과와 다를 수 있습니다.

다른 단어를 입력하여 유사성을 확인해보시면, 놀라울 정도로 유사한 단어를 찾는 경우도 있지만 이해하기 힘든 결과를 출력하는 경우도 있습니다. 이는 주제에 맞는 말뭉치 데이터가 부족해서 생기는 현상이니 품질 좋은 말뭉치 데이터를 학습하면 임베딩 성능이 좋아집니다.


<h2 align="center">Further</h2>

간단하게 input 내장함수를 사용하여 유사한 단어를 찾아주는 코드를 작성해 봅시다.

 > [🔍 코드 확인하러 가기](./codes/nvmc-model/find-similar-word.py)

저같은 경우는, '울버린'을 입력하고 8개의 유사한 단어를 찾아달라고 명령했습니다.

```
Enter any word that you want to find similar word : 울버린
How many similar words you want? : 8
[('고지라', 0.6919588446617126), ('퍼스트 클래스', 0.6855024099349976), ('X맨', 0.6837846636772156), ('엑스맨', 0.6686899662017822), ('착신아리', 0.6667824983596802), ('스코어', 0.6567636728286743), ('러브 액츄얼리', 0.6380522847175598), ('왜놈', 0.6359009742736816)]
```

그러니 위와 같은 결과가 나오더군요.


<h2 align="center">Finally</h2>

말뭉치의 품질과 데이터양이 충분하다면, 훌륭한 품질의 임베딩 모델을 구축할 수 있습니다. 이런 임베딩은 사람이 이해하는 정보를 컴퓨터가 이해할 숫 있도록 형태로 변환해주는 역할을 하기 때문에 일반적으로 신경망 모델의 입력으로 많이 사용됩니다.